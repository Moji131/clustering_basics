[
  {
    "objectID": "06_tsne.html",
    "href": "06_tsne.html",
    "title": "üåÄ t-SNE: t-Distributed Stochastic Neighbour Embedding",
    "section": "",
    "text": "t-SNE (t-Distributed Stochastic Neighbour Embedding) is a powerful tool for visualising high-dimensional data in 2 or 3 dimensions. Unlike PCA, which captures global variance linearly, t-SNE focuses on preserving local similarities ‚Äî so points that are close in high-dimensional space remain close in the visualisation.\n\n\nüß≠ How t-SNE Embeds Points in 2D\n\nComputes pairwise similarities in high-dimensional space using a Gaussian kernel. For each point \\(x_i\\), the similarity to \\(x_j\\) is defined as a conditional probability:\n\\[\np_{j|i} = \\frac{\\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\left(-\\frac{\\|x_i - x_k\\|^2}{2\\sigma_i^2}\\right)}\n\\]\nThe final joint probability is symmetrised as:\n\\[\np_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}\n\\]\nwhere \\(n\\) is the number of samples.\nRandomly initializes all points in 2D (or optionally uses PCA).\nComputes similarities in 2D using a Student‚Äôs t-distribution with 1 degree of freedom (Cauchy distribution), which has heavy tails:\n\\[\nq_{ij} = \\frac{\\left(1 + \\|y_i - y_j\\|^2\\right)^{-1}}{\\sum_{k \\neq l} \\left(1 + \\|y_k - y_l\\|^2\\right)^{-1}}\n\\]\nThis allows dissimilar points to be spread apart more in low-dimensional space, solving the ‚Äúcrowding problem‚Äù.\nMinimises the difference between \\(P\\) and \\(Q\\) by gradient descent.\n\nThis optimization process continues for many iterations (often 250‚Äì1000), adjusting point positions until a meaningful low-dimensional layout emerges.\n\n\n\nüîç When to Use t-SNE?\n\nTo visualise structure and clusters in high-dimensional data.\nWhen PCA doesn‚Äôt clearly separate groups.\nFor exploratory analysis ‚Äî axes don‚Äôt have interpretable meaning.\n\n‚ö†Ô∏è t-SNE is non-deterministic unless you fix the random_state, and computationally expensive, so it‚Äôs often applied after PCA for dimensionality reduction.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load and scale the data\ndata = load_wine()\nX = StandardScaler().fit_transform(data.data)\ny = data.target\n\n# Apply t-SNE with default settings\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\n# Visualise\nplt.figure(figsize=(6,6))\nsns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"Set1\", s=50)\nplt.title(\"t-SNE Projection (Default Parameters)\")\nplt.xlabel(\"t-SNE 1\")\nplt.ylabel(\"t-SNE 2\")\nplt.legend(title=\"Class\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüîç Default t-SNE Projection\nThis is a 2D projection of the wine dataset using t-SNE with default parameters.\nUnlike PCA, t-SNE focuses on maintaining the local neighbourhood structure, often revealing clusters more clearly ‚Äî even if the axes themselves don‚Äôt have interpretable meanings.\nThe colouring reflects the true wine cultivars (used only for validation, not during fitting).\n\n\nüîß Perplexity: Balancing Local vs Global Structure\nPerplexity controls how t-SNE balances local vs global relationships between data points.\n\nThink of it as the effective number of nearest neighbors used for each point.\nTypical values are between 5 and 50.\nSmaller perplexity emphasizes local details, while larger values consider a broader neighborhood.\n\nt-SNE adapts its similarity distribution to match this level of neighborhood complexity.\nLet‚Äôs see how different perplexity values affect the shape and separation of clusters.\n\nperplexities = [5, 20, 40, 60]\nfig, axes = plt.subplots(2, 2, figsize=(12,10))\n\nfor ax, perp in zip(axes.flat, perplexities):\n    tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"Set1\", s=40, ax=ax, legend=False)\n    ax.set_title(f\"Perplexity = {perp}\")\n    ax.set_xlabel(\"t-SNE 1\")\n    ax.set_ylabel(\"t-SNE 2\")\n\nplt.suptitle(\"Effect of Perplexity on t-SNE\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüîß Learning Rate: Controlling Step Size in Optimisation\nThe learning rate in t-SNE controls how fast the algorithm updates point positions during optimisation.\n\nA value too small may get stuck in poor local minima or collapse the points into tight blobs.\nA value too large may overshoot and produce scattered, noisy results.\nGood values often lie in the range 10 to 1000, depending on the dataset.\n\nLet‚Äôs visualise how different learning rates affect the projection.\n\nlearning_rates = [10, 100, 500, 1000]\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor ax, lr in zip(axes.flat, learning_rates):\n    tsne = TSNE(n_components=2, learning_rate=lr, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"Set1\", s=40, ax=ax, legend=False)\n    ax.set_title(f\"Learning Rate = {lr}\")\n    ax.set_xlabel(\"t-SNE 1\")\n    ax.set_ylabel(\"t-SNE 2\")\n\nplt.suptitle(\"Effect of Learning Rate on t-SNE\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüîß n_iter: Number of Optimisation Steps\nThe n_iter parameter sets how many iterations t-SNE runs to optimise the low-dimensional embedding.\n\nt-SNE starts with a random or PCA-based initial layout and iteratively adjusts it.\nMore iterations usually lead to better convergence, but with diminishing returns.\nValues below 250 may not converge well; ‚â•1000 is commonly used.\n\nLet‚Äôs explore how the number of iterations affects cluster stability and separation.\n\nn_iters = [250, 500, 1000, 2000]\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\nfor ax, n in zip(axes.flat, n_iters):\n    tsne = TSNE(n_components=2, max_iter=n, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n    sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=y, palette=\"Set1\", s=40, ax=ax, legend=False)\n    ax.set_title(f\"n_iter = {n}\")\n    ax.set_xlabel(\"t-SNE 1\")\n    ax.set_ylabel(\"t-SNE 2\")\n\nplt.suptitle(\"Effect of n_iter on t-SNE\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüîç Side-by-Side Comparison: Raw Features vs PCA vs t-SNE\nBelow we compare three ways of reducing dimensionality to 2D for visualisation:\n\nTwo Raw Features: No transformation ‚Äî simply plot two original dimensions.\nPCA (2 components): Linear transformation that captures maximum variance.\nt-SNE (2 components): Nonlinear projection that preserves local similarities.\n\nEach plot uses the same wine labels as color. This highlights how well-separated the clusters are under each method.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE\n\n# Clustering\ndef cluster_and_score(X_proj, name):\n    kmeans = KMeans(n_clusters=3, init=\"k-means++\", n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_proj)\n    score = silhouette_score(X_proj, labels)\n    print(f\"Silhouette Score ({name}): {score:.4f}\")\n    return labels\n\n# Fit\nX_raw = X[:, :2]\nX_pca = PCA(n_components=2).fit_transform(X)\nX_tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, max_iter=1000, random_state=42).fit_transform(X)\n\nlabels_raw = cluster_and_score(X_raw, \"Raw Features\")\nlabels_pca = cluster_and_score(X_pca, \"PCA\")\nlabels_tsne = cluster_and_score(X_tsne, \"t-SNE\")\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Raw\nsns.scatterplot(x=X_raw[:, 0], y=X_raw[:, 1], hue=labels_raw, palette=\"tab10\", s=50, ax=axes[0])\naxes[0].set_title(\"K-Means on Raw Features\")\naxes[0].set_xlabel(data.feature_names[0])\naxes[0].set_ylabel(data.feature_names[1])\n\n# PCA\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_pca, palette=\"tab10\", s=50, ax=axes[1])\naxes[1].set_title(\"K-Means on PCA\")\naxes[1].set_xlabel(\"PC1\")\naxes[1].set_ylabel(\"PC2\")\n\n# t-SNE\nsns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels_tsne, palette=\"tab10\", s=50, ax=axes[2])\naxes[2].set_title(\"K-Means on t-SNE\")\naxes[2].set_xlabel(\"t-SNE 1\")\naxes[2].set_ylabel(\"t-SNE 2\")\n\nplt.tight_layout()\nplt.show()\n\nSilhouette Score (Raw Features): 0.4841\nSilhouette Score (PCA): 0.5611\nSilhouette Score (t-SNE): 0.6071",
    "crumbs": [
      "06_tsne"
    ]
  },
  {
    "objectID": "04_dbscan_clustering.html",
    "href": "04_dbscan_clustering.html",
    "title": "üß≠ Density-Based Clustering with DBSCAN",
    "section": "",
    "text": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a powerful unsupervised clustering algorithm that groups together points that are densely packed and labels points in low-density regions as noise (outliers).\nUnlike K-Means or hierarchical clustering, DBSCAN does not require you to specify the number of clusters. Instead, it discovers clusters based on the idea of density connectivity.",
    "crumbs": [
      "04_dbscan_clustering"
    ]
  },
  {
    "objectID": "04_dbscan_clustering.html#demonstration-hierarchical-clustering-on-synthetic-data",
    "href": "04_dbscan_clustering.html#demonstration-hierarchical-clustering-on-synthetic-data",
    "title": "üß≠ Density-Based Clustering with DBSCAN",
    "section": "Demonstration: Hierarchical Clustering on Synthetic Data",
    "text": "Demonstration: Hierarchical Clustering on Synthetic Data\nWe‚Äôll generate a 2D dataset using make_blobs with 2 clusters and apply dbscan.\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n\n# Generate dataset with non-globular clusters\nX_synth, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n\n# Run DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=5)\nlabels = db.fit_predict(X_synth)\n\n# Visualise result\nplt.figure(figsize=(6,6))\nplt.scatter(X_synth[:, 0], X_synth[:, 1], c=labels, cmap=\"tab10\", s=30)\nplt.title(\"DBSCAN Clustering (Synthetic Data)\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()",
    "crumbs": [
      "04_dbscan_clustering"
    ]
  },
  {
    "objectID": "04_dbscan_clustering.html#clustering-the-wine-dataset",
    "href": "04_dbscan_clustering.html#clustering-the-wine-dataset",
    "title": "üß≠ Density-Based Clustering with DBSCAN",
    "section": "Clustering the Wine Dataset",
    "text": "Clustering the Wine Dataset\nWe now apply DBSCAN to the Wine dataset, which includes 13 chemical measurements of wines from 3 grape cultivars.\nUnlike K-Means, DBSCAN does not require knowing the number of clusters. Instead, we will tune key hyperparameters like eps, min_samples, and metric to get meaningful clusters.\nLet‚Äôs first load and inspect the data.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Load data\nwine = load_wine()\nX_raw = wine.data\ny_true = wine.target\nfeature_names = wine.feature_names\n\n# Standardise\nscaler = StandardScaler()\nX = scaler.fit_transform(X_raw)\n\n# Display\ndf_wine = pd.DataFrame(X_raw, columns=feature_names)\ndf_wine[\"Target\"] = y_true\ndf_wine.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nTarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\nHyperparameter: eps (Epsilon Neighborhood)\nThe eps parameter defines the radius for a point‚Äôs neighborhood. A point is considered a core point if at least min_samples points (including itself) fall within this radius.\n\nA small eps ‚Üí many points marked as noise\nA large eps ‚Üí clusters merge together\n\nWe‚Äôll test a range of eps values and evaluate clustering quality using the Silhouette Score.\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\n\neps_values = [ 1, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\nscores_eps = []\n\nfor eps in eps_values:\n    db = DBSCAN(eps=eps, min_samples=5, metric=\"euclidean\")\n    labels = db.fit_predict(X)\n\n    # Only evaluate if at least 2 clusters found\n    if len(set(labels)) &gt; 1 and -1 in labels:\n        score = silhouette_score(X, labels)\n    elif len(set(labels)) &gt; 1:\n        score = silhouette_score(X, labels)\n    else:\n        score = -1\n\n    scores_eps.append((eps, score))\n\ndf_eps = pd.DataFrame(scores_eps, columns=[\"eps\", \"Silhouette Score\"])\ndf_eps\n\nplt.figure(figsize=(6,4))\nplt.plot(df_eps[\"eps\"], df_eps[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"eps\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs eps\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter: min_samples\nThe min_samples parameter defines the minimum number of points required in a neighborhood (within radius eps) to consider a point a core point.\n\nLow min_samples ‚Üí more clusters, less noise\nHigh min_samples ‚Üí fewer clusters, more conservative grouping\n\nWe‚Äôll test a range of min_samples values with a fixed `ep_\n\nmin_samples_vals = [3, 5, 7, 10, 15]\nscores_min_samples = []\n\nfor m in min_samples_vals:\n    db = DBSCAN(eps=3.5, min_samples=m, metric=\"euclidean\")\n    labels = db.fit_predict(X)\n\n    if len(set(labels)) &gt; 1 and -1 in labels:\n        score = silhouette_score(X, labels)\n    elif len(set(labels)) &gt; 1:\n        score = silhouette_score(X, labels)\n    else:\n        score = -1\n\n    scores_min_samples.append((m, score))\n\ndf_mins = pd.DataFrame(scores_min_samples, columns=[\"min_samples\", \"Silhouette Score\"])\ndf_mins\n\nplt.figure(figsize=(6,4))\nplt.plot(df_mins[\"min_samples\"], df_mins[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"min_samples\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs min_samples\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHyperparameter: metric (Distance Function)\nThe metric parameter defines how distances between points are calculated. This affects how DBSCAN determines which points are close enough to form a cluster.\nCommon options include: - \"euclidean\": straight-line distance (default) - \"manhattan\": city-block distance - \"cosine\": angular similarity (often used in text embeddings)\nDifferent distance metrics can lead to very different clustering behaviour. We‚Äôll test several using eps = 0.5 and min_samples = 5.\n\nmetric_vals = [\"euclidean\", \"manhattan\", \"cosine\"]\nscores_metric = []\n\nfor metric in metric_vals:\n    db = DBSCAN(eps=3.5, min_samples=5, metric=metric)\n    labels = db.fit_predict(X)\n\n    if len(set(labels)) &gt; 1 and -1 in labels:\n        score = silhouette_score(X, labels)\n    elif len(set(labels)) &gt; 1:\n        score = silhouette_score(X, labels)\n    else:\n        score = -1\n\n    scores_metric.append((metric, score))\n\ndf_metric = pd.DataFrame(scores_metric, columns=[\"metric\", \"Silhouette Score\"])\ndf_metric\n\nplt.figure(figsize=(6,4))\nplt.plot(df_metric[\"metric\"], df_metric[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"Distance Metric\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs Distance Metric\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinal DBSCAN Model and Evaluation\nBased on our hyperparameter testing, we now run DBSCAN one last time using the best-performing combination.\nWe will: - Compute the Silhouette Score - Compare the resulting clusters to the true wine cultivars using a composition table\n\n# Best combination based on earlier testing (modify if different in your case)\ndb_final = DBSCAN(eps=3.5, min_samples=5, metric=\"euclidean\")\nfinal_labels = db_final.fit_predict(X)\n\n# Compute silhouette score\nif len(set(final_labels)) &gt; 1:\n    sil_score_final = silhouette_score(X, final_labels)\nelse:\n    sil_score_final = -1\n\nprint(f\"Silhouette Score (final DBSCAN): {sil_score_final:.4f}\")\nprint(\"Unique labels:\", set(final_labels))\n\nSilhouette Score (final DBSCAN): 0.2498\nUnique labels: {np.int64(0), np.int64(-1)}\n\n\n\n# Create comparison DataFrame\ndf_result = pd.DataFrame({\n    \"True Label\": y_true,\n    \"Cluster\": final_labels\n})\n\n# Crosstab (ignores noise: label -1)\ncomposition = pd.crosstab(df_result[\"Cluster\"], df_result[\"True Label\"],\n                          rownames=[\"Predicted Cluster\"], colnames=[\"True Cultivar\"])\ncomposition\n\n\n\n\n\n\n\nTrue Cultivar\n0\n1\n2\n\n\nPredicted Cluster\n\n\n\n\n\n\n\n-1\n0\n6\n0\n\n\n0\n59\n65\n48",
    "crumbs": [
      "04_dbscan_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html",
    "href": "02_kmeans_clustering.html",
    "title": "K-Means Clustering",
    "section": "",
    "text": "In this notebook, we‚Äôll dive deeper into K-Means, one of the most widely used clustering algorithms. We‚Äôll explore how it works, how to choose the number of clusters (k), and how to evaluate the clustering quality.\nWe‚Äôll use the Silhouette Score as our primary evaluation metric, along with visual inspection.",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#how-k-means-clustering-works",
    "href": "02_kmeans_clustering.html#how-k-means-clustering-works",
    "title": "K-Means Clustering",
    "section": "How K-Means Clustering Works",
    "text": "How K-Means Clustering Works\nK-Means is a centroid-based clustering algorithm that partitions the dataset into (k) clusters.\n\nSteps:\n\nInitialisation: Randomly place (k) centroids in the feature space.\nAssignment: Each data point is assigned to the nearest centroid, forming clusters.\nUpdate: New centroids are computed as the mean of all points in each cluster.\nRepeat: Assignment and update steps repeat until centroids stabilise or a maximum number of iterations is reached.\n\nThe algorithm aims to minimise the total within-cluster sum of squares (inertia), i.e., the variance within each cluster.\nK-Means works well for convex, isotropic clusters but struggles with non-spherical or unevenly sized groups.",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#k-means-hyperparameters-and-initialisation-strategies",
    "href": "02_kmeans_clustering.html#k-means-hyperparameters-and-initialisation-strategies",
    "title": "K-Means Clustering",
    "section": "K-Means: Hyperparameters and Initialisation Strategies",
    "text": "K-Means: Hyperparameters and Initialisation Strategies\n\nKey Hyperparameters\nApart from the number of clusters \\(k\\), the main hyperparameters in K-Means include:\n\ninit: How the initial centroids are selected.\n\nCommon values: \"random\" or \"k-means++\"\n\n\nn_init: Number of times the algorithm is run with different centroid seeds. The best result (lowest inertia) is chosen.\n\nDefault is 10 in Scikit-Learn.\n\nmax_iter: Maximum number of iterations allowed for a single run.\nrandom_state: Controls randomness in centroid initialisation for reproducibility.\n\n\n\n\nüìâ Inertia in K-Means\nInertia measures the total squared distance of each data point to its assigned cluster centroid. It represents how compact the clusters are.\n\\[\n\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n\\]\n\n\\(k\\): number of clusters\n\n\\(C_i\\): cluster \\(i\\)\n\n\\(\\mu_i\\): centroid of cluster \\(i\\)\n\n\\(x\\): data point in \\(C_i\\)\n\n\\(\\| x - \\mu_i \\|^2\\): squared Euclidean distance\n\nLower inertia means tighter, more coherent clusters.\n\n\nChoosing the Number of Clusters (\\(k\\))\nK-Means requires \\(k\\) to be specified in advance. There are two common methods to choose it:\n\nElbow Method:\nPlot the total within-cluster sum of squares (WCSS or inertia) against different \\(k\\) values. Look for an ‚Äúelbow‚Äù in the plot ‚Äî a point where adding more clusters doesn‚Äôt significantly reduce the inertia.\nSilhouette Score:\nEvaluate how well-separated and compact the clusters are. The \\(k\\) that gives the highest average silhouette score is often a good choice.\n\n\n\n\nk-means++ Initialisation\nInitialising centroids randomly can lead to poor convergence or suboptimal clustering. k-means++ is a smarter initialisation strategy that improves both speed and accuracy.\n\nHow it works:\n\nRandomly choose the first centroid from the data points.\nFor each remaining data point \\(x\\), compute its distance \\(D(x)\\) to the nearest already chosen centroid.\nSelect the next centroid from the remaining points with probability proportional to \\(D(x)^2\\).\nRepeat until \\(k\\) centroids are selected.\n\nThis spreads the initial centroids out in the data space and often results in better clustering.\nIn Scikit-Learn, this is enabled by default using init=\"k-means++\".\n\nIn practice, using init=\"k-means++\" with multiple n_init runs and evaluating with the silhouette score or elbow method gives reliable and stable results.",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#demonstration-k-means-on-synthetic-data",
    "href": "02_kmeans_clustering.html#demonstration-k-means-on-synthetic-data",
    "title": "K-Means Clustering",
    "section": "Demonstration: K-Means on Synthetic Data",
    "text": "Demonstration: K-Means on Synthetic Data\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport pandas as pd\n\nsns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#choosing-the-number-of-clusters",
    "href": "02_kmeans_clustering.html#choosing-the-number-of-clusters",
    "title": "K-Means Clustering",
    "section": "Choosing the Number of Clusters",
    "text": "Choosing the Number of Clusters\nWe‚Äôll run K-Means for different values of \\(k\\) and compute the average silhouette score to decide the best number of clusters.\n\n\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], s=30, color='grey', alpha=0.7)\nplt.title(\"Toy dataset with 4 true clusters\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()\n\n\n\n\n\n\n\n\n\nresults = []\nks = [2, 3, 4, 5, 6]\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X)\n    sil = silhouette_score(X, labels)\n    results.append((k, sil))\n\ndf = pd.DataFrame(results, columns=[\"k\", \"Silhouette\"])\ndisplay(df)\n\n# Plot silhouette score vs k\nplt.figure(figsize=(6,4))\nplt.plot(df.k, df.Silhouette, marker='o')\nplt.xlabel(\"Number of clusters (k)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score for Different k\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nk\nSilhouette\n\n\n\n\n0\n2\n0.615485\n\n\n1\n3\n0.799280\n\n\n2\n4\n0.875647\n\n\n3\n5\n0.731072\n\n\n4\n6\n0.585323",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#final-clustering-with-best-k",
    "href": "02_kmeans_clustering.html#final-clustering-with-best-k",
    "title": "K-Means Clustering",
    "section": "Final Clustering with Best k",
    "text": "Final Clustering with Best k\nFrom the silhouette scores, we choose the best \\(k\\) (typically where the score peaks). Let‚Äôs fit K-Means again and visualise the clusters.\n\nbest_k = 4\nkm = KMeans(n_clusters=best_k, random_state=42)\nlabels = km.fit_predict(X)\ncentroids = km.cluster_centers_\n\nplt.figure(figsize=(6,6))\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30, alpha=0.8)\nplt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='black', marker='X')\nplt.title(f\"K-Means Clustering with k={best_k}\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "02_kmeans_clustering.html#application-clustering-on-the-wine-dataset",
    "href": "02_kmeans_clustering.html#application-clustering-on-the-wine-dataset",
    "title": "K-Means Clustering",
    "section": "Application: Clustering on the Wine Dataset",
    "text": "Application: Clustering on the Wine Dataset\nIn this section, we‚Äôll apply K-Means clustering to the Wine dataset, a real-world dataset included in Scikit-Learn.\nThe dataset contains chemical properties of wines derived from three different cultivars (grape varieties). Each sample includes 13 numerical features such as: - Alcohol - Malic acid - Ash - Flavanoids - Color intensity\nAlthough the dataset has labels (the cultivar), we‚Äôll treat this as an unsupervised problem and see how well K-Means can discover the natural groupings.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Load dataset\nwine = load_wine()\nX_raw = wine.data\ny_true = wine.target\nfeature_names = wine.feature_names\n\n# Standardise features\nscaler = StandardScaler()\nX = scaler.fit_transform(X_raw)\n\ndf_wine = pd.DataFrame(X_raw, columns=feature_names)\ndf_wine['Target (Cultivar)'] = y_true\n\n# Display first few rows\ndf_wine.head()\n\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nTarget (Cultivar)\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\n\n\n\n\n\n\n\n\nHyperparameter: Number of Clusters (k)\nThe k parameter specifies the number of clusters that K-Means will try to find.\nSince the wine dataset has 3 known cultivars, we‚Äôll test a range of \\(k\\) values around that to see which yields the best silhouette score, which reflects how well-separated and compact the clusters are.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores_k = []\nks = range(2, 8)\n\nfor k in ks:\n    km = KMeans(n_clusters=k, init=\"k-means++\", n_init=10, random_state=42)\n    labels = km.fit_predict(X)\n    score = silhouette_score(X, labels)\n    silhouette_scores_k.append((k, score))\n\ndf_k = pd.DataFrame(silhouette_scores_k, columns=[\"k\", \"Silhouette Score\"])\ndf_k\n\nplt.figure(figsize=(6,4))\nplt.plot(df_k[\"k\"], df_k[\"Silhouette Score\"], marker='o', linestyle='-')\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs k\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter: Centroid Initialisation (init)\nThe init parameter controls how the initial centroids are selected:\n\n\"random\": choose k random points from the data\n\"k-means++\": spread out initial centroids for better results (default)\n\nPoor initialisation can lead to suboptimal clustering. Let‚Äôs compare the silhouette scores for both.\n\ninit_methods = [\"random\", \"k-means++\"]\nsilhouette_scores_init = []\n\nfor method in init_methods:\n    km = KMeans(n_clusters=3, init=method, n_init=10, random_state=42)\n    labels = km.fit_predict(X)\n    score = silhouette_score(X, labels)\n    silhouette_scores_init.append((method, score))\n\ndf_init = pd.DataFrame(silhouette_scores_init, columns=[\"Init Method\", \"Silhouette Score\"])\ndf_init\n\nplt.figure(figsize=(6,4))\nplt.plot(df_init[\"Init Method\"], df_init[\"Silhouette Score\"], marker='o', linestyle='-')\nplt.xlabel(\"Initialisation Method\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs Init Method\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHyperparameter: Number of Initialisations (n_init)\nK-Means is run multiple times with different random initialisations. The n_init parameter determines how many times this happens.\nHigher n_init increases robustness, but also computation time. We‚Äôll test a few values to see if performance improves.\n\nn_init_vals = [1, 5, 10, 20, 50]\nsilhouette_scores_ninit = []\n\nfor n in n_init_vals:\n    km = KMeans(n_clusters=3, init=\"k-means++\", n_init=n, random_state=42)\n    labels = km.fit_predict(X)\n    score = silhouette_score(X, labels)\n    silhouette_scores_ninit.append((n, score))\n\ndf_ninit = pd.DataFrame(silhouette_scores_ninit, columns=[\"n_init\", \"Silhouette Score\"])\ndf_ninit\n\nplt.figure(figsize=(6,4))\nplt.plot(df_ninit[\"n_init\"], df_ninit[\"Silhouette Score\"], marker='o', linestyle='-')\nplt.xlabel(\"n_init (Number of Restarts)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs n_init\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFinal K-Means Clustering with Selected Hyperparameters\nBased on our tests: - k = 3 yields the best clustering - init = \"k-means++\" provides better centroid spread - n_init = 10 is sufficient for stable results\nLet‚Äôs run K-Means one last time with these settings.\n\nbest_k = 3\nkm_final = KMeans(n_clusters=best_k, init=\"k-means++\", n_init=10, random_state=42)\nfinal_labels = km_final.fit_predict(X)\n\n# Optional: inspect how many points are in each cluster\npd.Series(final_labels).value_counts().sort_index()\n\nfrom sklearn.metrics import silhouette_score\nimport seaborn as sns\n\n# Compute silhouette score\nfinal_sil_score = silhouette_score(X, final_labels)\nprint(f\"Silhouette Score (final model): {final_sil_score:.4f}\")\n\n# Create a DataFrame for comparison\ndf_compare = pd.DataFrame({\n    \"True Label\": y_true,\n    \"Cluster\": final_labels\n})\n\n# Crosstab to see composition of each cluster by true label\ncluster_summary = pd.crosstab(df_compare[\"Cluster\"], df_compare[\"True Label\"],\n                              rownames=[\"Cluster\"], colnames=[\"True Cultivar\"])\ncluster_summary\n\n\nSilhouette Score (final model): 0.2849\n\n\n\n\n\n\n\n\nTrue Cultivar\n0\n1\n2\n\n\nCluster\n\n\n\n\n\n\n\n0\n0\n65\n0\n\n\n1\n0\n3\n48\n\n\n2\n59\n3\n0",
    "crumbs": [
      "02_kmeans_clustering"
    ]
  },
  {
    "objectID": "00_setup_guide.html",
    "href": "00_setup_guide.html",
    "title": "Clustering Basics",
    "section": "",
    "text": "Sign up for an NCI account if you don‚Äôt already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join.",
    "crumbs": [
      "00_setup_guide"
    ]
  },
  {
    "objectID": "00_setup_guide.html#nci-account-setup",
    "href": "00_setup_guide.html#nci-account-setup",
    "title": "Clustering Basics",
    "section": "",
    "text": "Sign up for an NCI account if you don‚Äôt already have one.\nSelect Projects and groups from the left hand side menu and then select the Find project or group tab. Search for cd82, the NCI-QCIF Training Partnership Project, and ask to join.",
    "crumbs": [
      "00_setup_guide"
    ]
  },
  {
    "objectID": "00_setup_guide.html#nci-australian-research-environment-are",
    "href": "00_setup_guide.html#nci-australian-research-environment-are",
    "title": "Clustering Basics",
    "section": "NCI Australian Research Environment (ARE)",
    "text": "NCI Australian Research Environment (ARE)\n\nConnect to NCI Australian Research Environment.\nBe sure you use your NCI ID (eg, ab1234) for the username and not your email address.\nUnder Featured Apps, find and click the JupterLab: Start a JupyterLab instance option. \nTo Launch a JuptyerLab session, set these resource requirements:\n\n\n\n\n\n\n\nResource\nValue\n\n\n\n\nWalltime (hours)\n5\n\n\nQueue\nnormalbw\n\n\nCompute Size\nsmall\n\n\nProject\ncd82\n\n\nStorage\nscratch/cd82\n\n\nAdvanced Options‚Ä¶\n\n\n\nModules\npython3/3.9.2\n\n\nPython or Conda virtual environment base\n/scratch/cd82/venv_workshop\n\n\n\nThen click the Launch button.\nThis will take you to your interactive session page you will see that that your JupyterLab session is Queued while ARE is searching for a compute node that will satisfy your requirements.\nOnce found, the page will update with a button that you can click to Open JupyterLab.\nHere is a screenshot of a JupyterLab landing page that should be similar to the one that opens in your web browser after starting the JupyterLab server on either macOS or Windows.",
    "crumbs": [
      "00_setup_guide"
    ]
  },
  {
    "objectID": "00_setup_guide.html#transferring-workshop-notebooks",
    "href": "00_setup_guide.html#transferring-workshop-notebooks",
    "title": "Clustering Basics",
    "section": "Transferring workshop notebooks",
    "text": "Transferring workshop notebooks\nWhen you have a Jupyter server running use JupyterLab file navigator to go the folder that has the same name as your username. Then make a new Jupyter notebook by clicking on the ‚ÄúPython 3‚Äù icon under ‚ÄúNotebook‚Äù section and run the following code in a cell:\n!rm -rf /scratch/cd82/$USER/notebooks\n!mkdir -p /scratch/cd82/$USER/notebooks\n!cp /scratch/cd82/clustering_ws/* /scratch/cd82/$USER/notebooks/\n!ls /scratch/cd82/$USER/notebooks/\nAnd then use the Jupyter file browser to navigate to the directory: /scratch/cd82/$USER/notebooks/ (where $USER is your NCI username)",
    "crumbs": [
      "00_setup_guide"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html",
    "href": "01_intro_to_clustering.html",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "",
    "text": "Clustering is an unsupervised learning method that groups similar data points based on feature similarity, rather than relying on labels. It helps uncover hidden patterns in data.\nWhy is it useful? - Discover customer segments in marketing data\n- Detect anomalies in network traffic or IoT data\n- Compress images by grouping similar colors\n- Identify disease subtypes in healthcare\n- Cluster news articles by topic\nIn this notebook: 1. Overview of clustering methods\n2. Overview of dimensionality reduction\n3. An interactive demo: K-means on a toy dataset\n4. Metrics to evaluate clustering results",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html#methods-covered",
    "href": "01_intro_to_clustering.html#methods-covered",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "Methods Covered",
    "text": "Methods Covered\n\nClustering Algorithms\n\nK-means\nPartitions data into (k) clusters by minimizing within-cluster variance.\nHierarchical Clustering\nBuilds nested clusters via agglomerative or divisive linkage.\nDBSCAN (Density-Based)\nFinds clusters of arbitrary shape and identifies noise.\n\n\n\nDimensionality Reduction\n\nPCA (Principal Component Analysis)\nLinearly projects data into directions that maximize variance.\nt-SNE (t-Distributed Stochastic Neighbor Embedding)\nNon-linear technique optimized for visual separation in low-D.\n\n\n\nEvaluation Metrics\n\nSilhouette Score (‚àí1 to 1): compactness vs separation\n\nDavies‚ÄìBouldin Index: average distance ratio of clusters\n\nVisual Inspection: critical for clustering quality\n\nLet‚Äôs start with a hands-on demo using k-means on a synthetic dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nimport pandas as pd\n\n# sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html#generate-synthetic-data",
    "href": "01_intro_to_clustering.html#generate-synthetic-data",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "Generate synthetic data",
    "text": "Generate synthetic data\n\n\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)\n\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], s=30, color='grey', alpha=0.7)\nplt.title(\"Toy dataset with 4 true clusters\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html#how-k-means-clustering-works",
    "href": "01_intro_to_clustering.html#how-k-means-clustering-works",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "How K-Means Clustering Works",
    "text": "How K-Means Clustering Works\nK-Means is a centroid-based clustering algorithm that partitions the dataset into (k) clusters.\n\nSteps:\n\nInitialisation: Randomly place (k) centroids in the feature space.\nAssignment: Each data point is assigned to the nearest centroid, forming clusters.\nUpdate: New centroids are computed as the mean of all points in each cluster.\nRepeat: Assignment and update steps repeat until centroids stabilise or a maximum number of iterations is reached.\n\nThe algorithm aims to minimise the total within-cluster sum of squares (inertia), i.e., the variance within each cluster.\nK-Means works well for convex, isotropic clusters but struggles with non-spherical or unevenly sized groups.",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html#clustering-evaluation-metrics",
    "href": "01_intro_to_clustering.html#clustering-evaluation-metrics",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "Clustering Evaluation Metrics",
    "text": "Clustering Evaluation Metrics\nTo assess clustering quality, we use a combination of numerical metrics and visual inspection. These metrics rely on Euclidean distance (Cartesian distance) to measure intra-cluster compactness and inter-cluster separation.\n\nüìè Silhouette Score\nThe silhouette score measures how well each point fits within its cluster compared to other clusters.\nTo compute a single silhouette score for the entire clustering result, we take the average silhouette score across all data points:\n\\[\nS = \\frac{1}{n} \\sum_{i=1}^{n} s_i\n\\]\nwhere: - \\(n\\) is the total number of samples, - \\(s_i\\) is the silhouette score for point \\(i\\), calculated as:\n\\[\ns_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}\n\\]\n\n\n\nüîπ What are \\(a_i\\) and \\(b_i\\)?\n\n\\(a_i\\) = average distance from point \\(i\\) to all other points in the same cluster\n‚Üí Measures how tightly clustered the point is with its own group (intra-cluster cohesion).\n\\(b_i\\) = average distance from point \\(i\\) to all points in the nearest different cluster\n‚Üí Measures how far away the point is from the closest alternative cluster (inter-cluster separation).\n\n\n\n\nüìä Interpreting the Score\n\n\\(S \\approx 1\\): Points are well-clustered and clearly separated.\n\\(S \\approx 0\\): Points lie between clusters ‚Äî ambiguous assignment.\n\\(S &lt; 0\\): Points may be misclassified, closer to another cluster than their own.\n\n\nThe silhouette score provides both a quantitative and intuitive measure of clustering quality.\n\n\nDavies‚ÄìBouldin Index\nThe DB index evaluates the average similarity between each cluster and its most similar one, where similarity is the ratio of intra-cluster dispersion to inter-cluster distance. It is computed as:\n\\[\nDB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\ne i} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)\n\\]\nwhere: - \\(\\sigma_i\\) = average distance of all points in cluster \\(i\\) to its centroid \\(c_i\\) - \\(d(c_i, c_j)\\) = Euclidean distance between centroids \\(c_i\\) and \\(c_j\\)\n\nLower is better\nA DB index less than 1 usually implies well-separated clusters\n\n\n\nVisual Inspection\nPlots of clustered data remain invaluable: - Reveal the shape, overlap, and structure of clusters - Help verify metric results and uncover edge cases - Especially useful when dimensionality is reduced (e.g., with PCA or t-SNE)\nConclusion: No single metric is sufficient on its own. Use a combination of quantitative scores and visual checks for a robust understanding of clustering performance.\n\n\nresults = [] \nks = [2, 3, 4, 5, 6]\n\nfor k in ks:\n    km = KMeans(n_clusters=k, random_state=42)\n    labels = km.fit_predict(X)\n    sil = silhouette_score(X, labels)\n    db = davies_bouldin_score(X, labels)\n    results.append((k, sil, db))\n\n# Show results\ndf = pd.DataFrame(results, columns=[\"k\", \"Silhouette\", \"Davies-Bouldin\"])\ndf\n\n\n\n\n\n\n\n\nk\nSilhouette\nDavies-Bouldin\n\n\n\n\n0\n2\n0.615485\n0.483750\n\n\n1\n3\n0.799280\n0.312296\n\n\n2\n4\n0.875647\n0.173674\n\n\n3\n5\n0.731072\n0.587863\n\n\n4\n6\n0.585323\n0.844187\n\n\n\n\n\n\n\n\n# Plot results\nfig, ax1 = plt.subplots(figsize=(7, 4))\nax1.plot(df.k, df.Silhouette, marker='o', label='Silhouette')\nax1.plot(df.k, df[\"Davies-Bouldin\"], marker='o', label='Davies-Bouldin')\n\nax1.set_xlabel(\"Number of clusters (k)\")\nfig.tight_layout()\nplt.title(\"Clustering evaluation vs k\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nbest_k = 4\nkm4 = KMeans(n_clusters=best_k, random_state=42)\nlabels4 = km4.fit_predict(X)\ncentroids = km4.cluster_centers_\n\nplt.figure(figsize=(6,6))\nplt.scatter(X[:,0], X[:,1], c=labels4, s=30, cmap='tab10', alpha=0.8)\nplt.scatter(centroids[:,0], centroids[:,1], s=200, c='black', marker='X')\nplt.title(\"K-means clustering with k=4\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüßê Interpretation\nLet‚Äôs break down what each evaluation metric tells us:\n\nSilhouette Score: A score ‚â• 0.5 typically suggests well-separated, compact clusters. Here, the silhouette score peaks at (k = 4), suggesting it‚Äôs a strong candidate for the correct number of clusters.\nDavies‚ÄìBouldin Index: Lower values indicate better clustering. At (k = 4), the DB index is at its minimum, meaning the clusters are more distinct from one another and internally tight.\nVisual Inspection: When plotted, the clusters at (k = 4) appear well-formed and evenly separated. Visual inspection validates the metrics and helps catch edge cases where automated scores might be misleading.\n\nAll metrics point to (k = 4) as the optimal choice for this dataset, confirming that the clustering structure is meaningful and the method performed well.",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "01_intro_to_clustering.html#visualising-the-final-clustering-result-k-4",
    "href": "01_intro_to_clustering.html#visualising-the-final-clustering-result-k-4",
    "title": "Introduction to Clustering and Dimensionality Reduction",
    "section": "Visualising the Final Clustering Result (k = 4)",
    "text": "Visualising the Final Clustering Result (k = 4)\nBased on our evaluation metrics, we identified k = 4 as the best number of clusters. In this step, we re-fit the K-Means model using k = 4 and visualise the resulting clusters.\nEach point is coloured according to its assigned cluster, and the cluster centroids are shown as large black ‚ÄúX‚Äù markers. This plot helps us visually confirm whether the clusters are: - Well-separated - Roughly equal in size - Centered around meaningful centroids\nIt‚Äôs a simple but powerful way to interpret and validate clustering results.\n\nbest_k = 3\nkm4 = KMeans(n_clusters=best_k, random_state=42)\nlabels4 = km4.fit_predict(X)\ncentroids = km4.cluster_centers_\n\nplt.figure(figsize=(6,6))\nplt.scatter(X[:,0], X[:,1], c=labels4, s=30, cmap='tab10', alpha=0.8)\nplt.scatter(centroids[:,0], centroids[:,1], s=200, c='black', marker='X')\nplt.title(\"K-means clustering with k=3\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()",
    "crumbs": [
      "01_intro_to_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html",
    "href": "03_hierarchical_clustering.html",
    "title": "üå≥ Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical Clustering is an unsupervised learning technique that builds a tree-like structure of nested clusters, called a dendrogram.\nUnlike K-Means, hierarchical clustering does not require you to choose the number of clusters upfront. Instead, it starts by either: - Agglomerative (bottom-up): each data point starts in its own cluster, and pairs of clusters are merged step-by-step. - (Less common) Divisive (top-down): starts with one big cluster and recursively splits it.",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html#demonstration-hierarchical-clustering-on-synthetic-data",
    "href": "03_hierarchical_clustering.html#demonstration-hierarchical-clustering-on-synthetic-data",
    "title": "üå≥ Hierarchical Clustering",
    "section": "Demonstration: Hierarchical Clustering on Synthetic Data",
    "text": "Demonstration: Hierarchical Clustering on Synthetic Data\nWe‚Äôll generate a 2D dataset using make_blobs with 3 obvious clusters and apply hierarchical clustering using the ward linkage method (which minimizes variance).\n\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\n\nsns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n\n# Generate synthetic dataset\nX_synth, y_synth = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=42)\n\n# Fit hierarchical clustering\nhc = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\nlabels_synth = hc.fit_predict(X_synth)\n\n# Visualise\nplt.figure(figsize=(6,6))\nplt.scatter(X_synth[:, 0], X_synth[:, 1], c=labels_synth, cmap=\"tab10\", s=30)\nplt.title(\"Hierarchical Clustering on Synthetic Data\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.show()\n\n# Evaluate\nprint(\"Silhouette Score:\", silhouette_score(X_synth, labels_synth))\n\n\n\n\n\n\n\n\nSilhouette Score: 0.9083834454815235\n\n\n\nüåø Dendrogram\nThe dendrogram represents the hierarchical merging of clusters based on their distance.\n\nEach leaf represents a data point (or cluster of points).\nThe height of each U-shaped link represents the distance (or dissimilarity) between merged clusters.\nBy drawing a horizontal line across the dendrogram, you can ‚Äúcut‚Äù it at a chosen height to determine the number of clusters.\n\nIn our case, we used Ward linkage, which merges clusters to minimize variance within each cluster.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Compute the linkage matrix using Ward's method\nlinked = linkage(X_synth, method='ward')\n\n# Plot the dendrogram\nplt.figure(figsize=(10, 5))\ndendrogram(linked,\n           truncate_mode='lastp',  # show only the last p merged clusters\n           p=30,\n           leaf_rotation=90.,\n           leaf_font_size=10.,\n           show_contracted=True)\nplt.title(\"Dendrogram (Ward Linkage)\")\nplt.xlabel(\"Sample Index or Cluster Size\")\nplt.ylabel(\"Distance\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html#clustering-the-wine-dataset",
    "href": "03_hierarchical_clustering.html#clustering-the-wine-dataset",
    "title": "üå≥ Hierarchical Clustering",
    "section": "Clustering the Wine Dataset",
    "text": "Clustering the Wine Dataset\nWe‚Äôll now apply hierarchical clustering to the Wine dataset, which includes 13 chemical features of wines from 3 different grape cultivars.\nOur goal is to see if hierarchical clustering can discover meaningful groupings in the data, without using the true labels.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Load and scale the data\nwine = load_wine()\nX_raw = wine.data\ny_true = wine.target\nfeature_names = wine.feature_names\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X_raw)\n\n# Create DataFrame\ndf_wine = pd.DataFrame(X_raw, columns=feature_names)\ndf_wine[\"Target\"] = y_true\ndf_wine.head()\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\nTarget\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html#hyperparameter-number-of-clusters-n_clusters",
    "href": "03_hierarchical_clustering.html#hyperparameter-number-of-clusters-n_clusters",
    "title": "üå≥ Hierarchical Clustering",
    "section": "Hyperparameter: Number of Clusters (n_clusters)",
    "text": "Hyperparameter: Number of Clusters (n_clusters)\nThis parameter determines how many clusters to extract from the dendrogram.\n\nIf n_clusters is too low, very different groups may be merged together.\nIf it‚Äôs too high, the algorithm might break up natural clusters into smaller subgroups.\nThe Silhouette Score can help us pick a good value by balancing cohesion and separation.\n\nWe will test a range of n_clusters values and select the best one using Silhouette Score.\n\ncluster_range = range(2, 8)\nresults_k = []\n\nfor k in cluster_range:\n    hc = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n    labels = hc.fit_predict(X)\n    score = silhouette_score(X, labels)\n    results_k.append((k, score))\n\ndf_k = pd.DataFrame(results_k, columns=[\"n_clusters\", \"Silhouette Score\"])\ndf_k\n\nplt.figure(figsize=(6,4))\nplt.plot(df_k[\"n_clusters\"], df_k[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs Number of Clusters\")\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html#hyperparameter-linkage-method-linkage",
    "href": "03_hierarchical_clustering.html#hyperparameter-linkage-method-linkage",
    "title": "üå≥ Hierarchical Clustering",
    "section": "Hyperparameter: Linkage Method (linkage)",
    "text": "Hyperparameter: Linkage Method (linkage)\nThe linkage method controls how the distance between clusters is defined when merging them:\n\n\"ward\": minimizes the total within-cluster variance (works only with Euclidean distance)\n\"complete\": uses the maximum pairwise distance (tends to create compact clusters)\n\"average\": uses the average pairwise distance\n\"single\": uses the minimum pairwise distance (can lead to chaining effects)\n\nDifferent linkage strategies can lead to very different cluster shapes and behaviours.\n\nüîó Ward Linkage (Variance Minimization)\nWard linkage is a popular method used in hierarchical (agglomerative) clustering. Unlike other linkage methods that rely purely on distances between points, Ward‚Äôs method focuses on minimizing the variance within each cluster.\nAt each step, it merges the pair of clusters that results in the smallest possible increase in the total within-cluster variance, also known as inertia.\n\nüßÆ Inertia Formula:\nThe within-cluster sum of squares (inertia) is given by:\n\\[\n\\text{Inertia} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n\\]\nwhere: - \\(k\\) is the number of clusters - \\(x\\) is a data point in cluster \\(C_i\\) - \\(\\mu_i\\) is the centroid of cluster \\(C_i\\) - \\(\\| x - \\mu_i \\|^2\\) is the squared Euclidean distance\n\n\n‚úÖ Characteristics:\n\nProduces compact, spherical clusters\nSimilar to what K-Means produces\nTends to avoid chaining and imbalanced cluster sizes\n\n\n\n‚ö†Ô∏è Limitation:\nWard linkage only supports Euclidean distance, since it is based on variance and centroids.\n\nIn summary:\nWard linkage is a strong default choice for hierarchical clustering when you expect well-separated, compact clusters and are working in Euclidean space.\n\nlinkage_methods = [\"ward\", \"complete\", \"average\", \"single\"]\nresults_linkage = []\n\nfor method in linkage_methods:\n    if method == \"ward\":\n        hc = AgglomerativeClustering(n_clusters=3, linkage=method)\n    else:\n        hc = AgglomerativeClustering(n_clusters=3, linkage=method)\n    labels = hc.fit_predict(X)\n    score = silhouette_score(X, labels)\n    results_linkage.append((method, score))\n\ndf_linkage = pd.DataFrame(results_linkage, columns=[\"Linkage\", \"Silhouette Score\"])\ndf_linkage\n\nplt.figure(figsize=(6,4))\nplt.plot(df_linkage[\"Linkage\"], df_linkage[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"Linkage Method\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs Linkage Method\")\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "03_hierarchical_clustering.html#hyperparameter-distance-metric-metric",
    "href": "03_hierarchical_clustering.html#hyperparameter-distance-metric-metric",
    "title": "üå≥ Hierarchical Clustering",
    "section": "Hyperparameter: Distance Metric (metric)",
    "text": "Hyperparameter: Distance Metric (metric)\nThe metric parameter determines how distances between samples are calculated.\n\n\"euclidean\": default for most use cases and required with ward linkage\n\"manhattan\": based on city-block distance\n\"cosine\": based on angular distance (useful for text or directional data)\n\nNot all metric values are compatible with all linkage methods. For example, ward only works with euclidean.\nWe‚Äôll try a few combinations of metric and linkage to see how they impact clustering results.\n\nmetric_methods = [\"euclidean\", \"manhattan\", \"cosine\"]\nresults_metric = []\n\nfor metric in metric_methods:\n    hc = AgglomerativeClustering(n_clusters=3, linkage=\"average\", metric=metric)\n    labels = hc.fit_predict(X)\n    score = silhouette_score(X, labels)\n    results_metric.append((metric, score))\n\ndf_metric = pd.DataFrame(results_metric, columns=[\"metric\", \"Silhouette Score\"])\ndf_metric\n\nplt.figure(figsize=(6,4))\nplt.plot(df_metric[\"metric\"], df_metric[\"Silhouette Score\"], marker='o')\nplt.xlabel(\"metric (Distance Metric)\")\nplt.ylabel(\"Silhouette Score\")\nplt.title(\"Silhouette Score vs metric\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nFinal Model and Evaluation\nBased on our tests, we‚Äôll use the best combination of: - n_clusters = 3 - linkage = ward - metric = euclidean\nLet‚Äôs run the final clustering and evaluate its silhouette score and compare clusters with true wine cultivars.\n\nhc_final = AgglomerativeClustering(n_clusters=3, linkage=\"ward\", metric=\"euclidean\")\nfinal_labels = hc_final.fit_predict(X)\n\n# Silhouette score\nscore_final = silhouette_score(X, final_labels)\nprint(\"Final Silhouette Score:\", score_final)\n\n# Cluster composition table\ndf_compare = pd.DataFrame({\n    \"True Label\": y_true,\n    \"Cluster\": final_labels\n})\n\ncomposition = pd.crosstab(df_compare[\"Cluster\"], df_compare[\"True Label\"],\n                          rownames=[\"Cluster\"], colnames=[\"True Cultivar\"])\ncomposition\n\nFinal Silhouette Score: 0.2774439826952266\n\n\n\n\n\n\n\n\nTrue Cultivar\n0\n1\n2\n\n\nCluster\n\n\n\n\n\n\n\n0\n0\n58\n0\n\n\n1\n0\n8\n48\n\n\n2\n59\n5\n0",
    "crumbs": [
      "03_hierarchical_clustering"
    ]
  },
  {
    "objectID": "05_PCA.html",
    "href": "05_PCA.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms the dataset into a new coordinate system, where the axes (principal components) are chosen to maximise variance.\nPCA helps us: - Reduce the number of features (dimensionality) - Decorrelate the features (remove redundancy) - Visualise high-dimensional data in 2D or 3D - Improve the performance of models like clustering\n\n\nüìê Key Idea\nGiven a dataset with \\(n\\) observations and \\(d\\) features, PCA finds new axes: - \\(PC_1\\): direction of maximum variance - \\(PC_2\\): orthogonal to \\(PC_1\\), with the next highest variance - and so on‚Ä¶\nEach principal component is a linear combination of the original features.\n\n\n\nüßÆ Mathematical Definition\nLet: - \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the mean-centred data matrix - \\(\\Sigma = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X}\\) be the empirical covariance matrix\nWe solve the eigenvalue decomposition problem: \\[\n\\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\n\\]\nWhere: - \\(\\mathbf{v}_i\\) is the \\(i\\)-th principal component direction - \\(\\lambda_i\\) is the variance explained by that direction\nThe components are ranked by \\(\\lambda_i\\) (largest to smallest).\n\n\n\nüìä Explained Variance\nThe explained variance ratio of each component is: \\[\n\\text{ExplainedVariance}(PC_i) = \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\n\\]\nWe often choose the first \\(k\\) components such that: \\[\n\\sum_{i=1}^{k} \\frac{\\lambda_i}{\\sum_{j=1}^{d} \\lambda_j} \\geq 0.95\n\\] This means we retain at least 95% of the original information.\n\n\n\nüîé Summary\n\nPCA is unsupervised and does not use class labels\nIt works best when the features are standardised\nIt can reveal structure in high-dimensional data and improve clustering or classification\n\nWe‚Äôll now apply PCA to real data and visualise its effects.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n# Load and scale\ndata = load_wine()\nX = StandardScaler().fit_transform(data.data)\ny = data.target\n\n# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nX_two = X[:, :2]\nfeature_x = data.feature_names[0]\nfeature_y = data.feature_names[1]\n\n\n# Define a custom color palette: red, yellow, blue\ncustom_palette = {0: \"red\", 1: \"gold\", 2: \"blue\"}  # Class 1 = yellow\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# PCA plot\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette=custom_palette, s=50, ax=axes[0])\naxes[0].set_title(\"PCA Projection (2 Components)\")\naxes[0].set_xlabel(\"PC1\")\naxes[0].set_ylabel(\"PC2\")\naxes[0].legend(title=\"Class\")\n\n# Original features plot\nsns.scatterplot(x=X_two[:, 0], y=X_two[:, 1], hue=y, palette=custom_palette, s=50, ax=axes[1])\naxes[1].set_title(f\"Original Features: {feature_x} vs {feature_y}\")\naxes[1].set_xlabel(feature_x)\naxes[1].set_ylabel(feature_y)\naxes[1].legend(title=\"Class\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nüí° Interpreting PC1\nThe first principal component (PC1) is a weighted combination of all original features. The weights (also called ‚Äúloadings‚Äù) tell us how much each feature contributes to the component.\nWe can interpret PC1 by examining which features have the highest contribution.\nNote: The direction (positive or negative) is important when interpreting actual component values, but for contribution magnitude, we consider the absolute values.\n\n# Get original feature names\nfeatures = data.feature_names\n\n# Extract PC1 loadings\npc1_loadings = pca.components_[0]\n\n# Create DataFrame of feature contributions to PC1\npc1_df = pd.DataFrame({\n    \"Feature\": features,\n    \"PC1 Weight\": pc1_loadings,\n    \"Contribution (%)\": 100 * pc1_loadings**2 / np.sum(pc1_loadings**2)\n}).sort_values(\"Contribution (%)\", ascending=False)\n\n# Display top contributors\npc1_df\n\nplt.figure(figsize=(8, 5))\nsns.barplot(data=pc1_df, y=\"Feature\", x=\"Contribution (%)\")\nplt.title(\"Feature Contributions to PC1\")\nplt.xlabel(\"Contribution (%)\")\nplt.ylabel(\"\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nüìà Explained Variance vs Number of Components\nThis plot shows the cumulative explained variance as we add more principal components.\n\nEach principal component captures a portion of the total variance in the data.\nThe curve tells us how many components are needed to retain most of the information.\nFor example, if the curve crosses 0.95 at 7 components, then keeping the first 7 components preserves 95% of the data‚Äôs structure.\n\nWe often use this plot to choose the optimal number of components for dimensionality reduction ‚Äî balancing accuracy and simplicity.\n\nimport numpy as np\n\npca_all = PCA().fit(X)\nplt.plot(np.cumsum(pca_all.explained_variance_ratio_), marker='o')\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.title(\"Explained Variance vs Components\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Clustering on original data\nlabels_orig = KMeans(n_clusters=3, random_state=42).fit_predict(X)\nscore_orig = silhouette_score(X, labels_orig)\n\n# Clustering on PCA-transformed data\nX_pca3 = PCA(n_components=3).fit_transform(X)\nlabels_pca = KMeans(n_clusters=3, random_state=42).fit_predict(X_pca3)\nscore_pca = silhouette_score(X_pca3, labels_pca)\n\nprint(f\"Silhouette Score (Original): {score_orig:.3f}\")\nprint(f\"Silhouette Score (PCA): {score_pca:.3f}\")\n\nSilhouette Score (Original): 0.285\nSilhouette Score (PCA): 0.454\n\n\n\n\nPCA with n_components=0.95\nInstead of manually choosing how many principal components to keep, we can set n_components=0.95. This tells PCA to select the minimum number of components that preserve at least 95% of the total variance in the data.\nWe‚Äôll transform the data using this setting, then apply K-Means clustering and visualise the results using the first two PCs.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Apply PCA to retain 95% variance\npca_95 = PCA(n_components=0.95)\nX_pca_95 = pca_95.fit_transform(X)\n\nprint(f\"Number of components selected to retain 95% variance: {X_pca_95.shape[1]}\")\n\n# Apply KMeans on reduced data\nkmeans_95 = KMeans(n_clusters=3, random_state=42)\nlabels_pca_95 = kmeans_95.fit_predict(X_pca_95)\n\n# Silhouette Score\nscore_pca_95 = silhouette_score(X_pca_95, labels_pca_95)\nprint(f\"Silhouette Score (PCA 95%): {score_pca_95:.3f}\")\n\n\nNumber of components selected to retain 95% variance: 10\nSilhouette Score (PCA 95%): 0.299\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Clustering\ndef cluster_and_score(X_proj, name):\n    kmeans = KMeans(n_clusters=3, init=\"k-means++\", n_init=10, random_state=42)\n    labels = kmeans.fit_predict(X_proj)\n    score = silhouette_score(X_proj, labels)\n    print(f\"Silhouette Score ({name}): {score:.4f}\")\n    return labels\n\n# Fit\nX_raw = X[:, :2]\nX_pca = PCA(n_components=2).fit_transform(X)\n\nlabels_raw = cluster_and_score(X_raw, \"Raw Features\")\nlabels_pca = cluster_and_score(X_pca, \"PCA\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(18, 5))\n\n# Raw\nsns.scatterplot(x=X_raw[:, 0], y=X_raw[:, 1], hue=labels_raw, palette=\"tab10\", s=50, ax=axes[0])\naxes[0].set_title(\"K-Means on Raw Features\")\naxes[0].set_xlabel(data.feature_names[0])\naxes[0].set_ylabel(data.feature_names[1])\n\n# PCA\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels_pca, palette=\"tab10\", s=50, ax=axes[1])\naxes[1].set_title(\"K-Means on PCA\")\naxes[1].set_xlabel(\"PC1\")\naxes[1].set_ylabel(\"PC2\")\n\n\nplt.tight_layout()\nplt.show()\n\nSilhouette Score (Raw Features): 0.4841\nSilhouette Score (PCA): 0.5611",
    "crumbs": [
      "05_PCA"
    ]
  }
]